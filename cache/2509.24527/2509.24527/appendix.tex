% \section{Supplementary Material}

\section{Datasets}

\paragraph{Minecraft VPT}

We use the OpenAI VPT dataset of contractor gameplay \citep{vpt} and combine the available subsets 6--10, resulting in 2541 hours of gameplay.
We split the dataset into 90\% training and 10\% evaluation data, ensuring that the splits do not share any of the same underlying 5-min recording chunks.
We encode keyboard actions as a vector of binary variables and process the mouse actions as in VPT by $\mu$-law encoding, discretizing into 11 bins per coordinate, and enumerating all $11 \times 11 = 121$ combinations to obtain a categorical variable.
The image resolution is $360 \times 640$ and the framerate is 20 FPS.
We zero pad the frames to $384 \times 640$ and then patchify with patch size $16 \times 16$ into 960 tokens.
We reshape the $(N_\mathrm{b} = 512) \times (D_\mathrm{b} = 16)$ bottleneck of the tokenizer to $(N_\mathrm{z} = 256) \times 32$ for the dynamics model.
We train the dynamics model with $N_\mathrm{z} = 256$ spatial tokens, context length $C = 192$, and batch lengths $T_1 = 64$ and $T_2 = 256$.

\paragraph{Minecraft Overworld and Nether split}

To study out-of-distribution generalization of action conditioning in \method, we carefully split the Minecraft dataset into videos of the Overworld versus the Nether dimension.
We also include the End dimension into the Nether portion of the dataset.
Both the Nether and the End feature unique visuals, blocks, and terrain shapes compared to the Overworld.
The Overworld includes natural landscapes with forests, deserts, oceans, and more, whereas the Nether is underworld-themed with red blocks and lava and the End is a space-themed region.
To separate the dataset, we want to ensure no leakage from players entering the Nether/End dimensions and bringing blocks from there back to the Overworld.
For this reason, we exclude the VPT 6 and 7 subsets, which contain long free play.
We then assigned each 5 min recording of the remaining dataset to either the Overworld or the Nether/End portion based on item events that are provided by the dataset.
Whenever a Nether/End item was interacted with, we assign that video to the Nether/End split.
This ensures that the Overworld split contains no Nether/End episodes, whereas the Nether/End split can sometimes contain some Overworld episodes, although this was rare in practice.
We manually investigated the Overworld split obtained by this strategy and found no Nether/End trajectories in it.

\paragraph{SOAR Robotics}

The SOAR dataset \citep{soar} contains teleoperated demonstrations and online trajectories of a reinforcement learning policy, thus covering both successes and failures.
We split the dataset into 90\% training and 10\% evaluation data.
The dataset contains a total of 180 hours of videos with 7D relative end-effector actions.
The image resolution is $256 \times 256$ and the framerate is 5 FPS.
We patchify with patch size $16 \times 16$ into 256 tokens.
We train the dynamics model with $N_\mathrm{z} = 512$ spatial tokens, context length $C = 96$, and batch lengths $T_1 = 32$ and $T_2 = 128$.

\paragraph{Epic Kitchens}

The Epic Kitchens 100 dataset \citep{epickitchens} contains 100 hours of video from the first-person perspective of humans across 45 kitchens.
The test set contains different tasks performed in the same kitchens.
We use the dataset at $256 \times 256$ resolution and 10 FPS.
We patchify with patch size $16 \times 16$ into 256 tokens.
We train the dynamics model with $N_\mathrm{z} = 512$ spatial tokens, context length $C = 96$, and batch lengths $T_1 = 32$ and $T_2 = 128$.

\clearpage
\section{Kitchen Generations}
\vfill
\input{figures/kitchens_more/figure}
\vfill

\section{Minecraft Tasks}
\vspace*{-2ex}
\begin{minipage}[t]{0.48\textwidth}
\input{tables/mc_tasks}
\vspace*{3ex}
\input{tables/mc_items}
\end{minipage}
\hfill%
\begin{minipage}[t]{0.48\textwidth}
\input{tables/mc_ladder}
\end{minipage}
\clearpage

\section{Offline Diamond Challenge}
\input{tables/rl_success}
\input{tables/rl_timing}
\clearpage

\enlargethispage{\baselineskip}
\vspace*{-9ex}
\section{Minecraft Inputs}
\vspace*{-1ex}
\input{figures/inputs/figure}

\section{Previous Dreamer Generations}
\vspace*{-1ex}
Dreamer~3\citep{dreamerv3} learned to obtain diamonds in Minecraft from scratch by online interaction.
Its inputs are low-resolution images and inventory states and the outputs are mouse, keyboard, and abstract crafting actions.
Dreamer~3 uses a recurrent state-space model (RSSM) \citep{hafner2018planet} as its world model, which is based on a recurrent neural network and a variational objective.
This approach results in a lightweight world model with highly efficient inference but is difficult to scale to diverse data distributions.
In contrast, \method learns to obtain diamonds in Minecraft purely from offline data.
Its inputs are only high-resolution images and the outputs are low-level mouse and keyboard actions.
\method uses a scalable world model based on an efficient transformer architecture and a shortcut forcing objective, allowing it to scale to diverse data distributions with many details.
While Dreamer~3 uses return normalization and an entropy regularizer, \method uses PMPO with a KL to the behavioral cloning prior for imagination training, where no normalization is needed.
\vspace*{1ex}
\input{figures/openl/figure}
\clearpage

\enlargethispage{\baselineskip}
\vspace*{-10ex}
\section{Human Interaction: Lucid-v1}
\vspace*{-2ex}
\input{figures/wmtasks/figure_lucid}
\vspace*{-2ex}
\clearpage

\enlargethispage{\baselineskip}
\vspace*{-10ex}
\section{Human Interaction: OASIS (large)}
\vspace*{-2ex}
\input{figures/wmtasks/figure_oasis}
\vspace*{-2ex}
\clearpage

\enlargethispage{\baselineskip}
\vspace*{-10ex}
\section{Human Interaction: \method}
\vspace*{-2ex}
\input{figures/wmtasks/figure_ours}
\clearpage
