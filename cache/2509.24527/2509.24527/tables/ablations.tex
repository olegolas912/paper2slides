\begin{table}[tb!]
\centering
\vspace*{-2ex}
\begin{mytabular}{
  colspec = {| L{15em} | C{5.5em} C{5.5em} C{5.5em} |},
  row{1} = {font=\bfseries},
  stretch=0.9,
}

\toprule
Model & Train step seconds & Inference FPS (\rlap{$\uparrow$)} & Quality FVD (\rlap{$\downarrow$)} \\
\midrule

Diffusion Forcing Transformer               & 9.8 & \o0.8 &  306 \\  % model1
+ Fewer sampling steps ($K=4$)              & 9.8 & \o9.1 &  875 \\  % model1
+ Shortcut model                            & 9.8 & \o9.1 &  329 \\  % model2
+ X-Prediction                              & 9.8 & \o9.1 &  326 \\  % model3
+ X-Loss                                    & 9.8 & \o9.1 &  151 \\  % model4
+ Ramp weight                               & 9.8 & \o9.1 &  102 \\  % model5
+ Alternating batch lengths                 & 1.5 & \o9.1 & \o80 \\  % model6
+ Long context every 4 layers               & 0.6 &  18.9 & \o70 \\  % model7
+ GQA                                       & 0.5 &  23.2 & \o71 \\  % model8
+ Time factorized long context              & 0.4 &  30.1 & \o91 \\  % model9
+ Register tokens                           & 0.5 &  28.9 & \o91 \\  % model10
+ More spatial tokens ($N_\mathrm{z}=128$)  & 0.8 &  25.7 & \o66 \\  % model11
+ More spatial tokens ($N_\mathrm{z}=256$)  & 1.7 &  21.4 & \o57 \\  % model12
\bottomrule

% Ours with vpred: 123.75

\end{mytabular}
\caption{
Cascade of model design choices.
\method is based on a shortcut forcing objective and an efficient transformer architecture, combining a range of known techniques to achieve accurate and fast interleaved generation.
Starting from a naive diffusion forcing transformer with $N_\mathrm{z}=64$ spatial tokens and $K=64$ sampling steps, we apply the objective and architecture modifications, and increase the number of spatial tokens once feasible.
Inference speed measured on a single H100 GPU.
The resulting world model achieves high model capacity and inference efficiency.
}
\label{tab:ablations}
\end{table}
